{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Source AFO\n",
    "#1 for supporting, 0 for rejecting, 2 for irrelevant\n",
    "#300 key words, 77.5% accuracy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bs4 as bs\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = pd.read_csv(\"GlobalWarming abstracts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>FullContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1991,A 20-year Record Of Alpine Grasshopper Ab...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1991,A Geological Perspective On Climatic-chan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1991,A Spatial Model For Studying The Effects ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1991,Abrupt Deep-sea Warming| Palaeoceanograph...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1991,Advance Of East Antarctic Outlet Glaciers...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title Abstract  sentiment  \\\n",
       "0  1991,A 20-year Record Of Alpine Grasshopper Ab...      NaN          4   \n",
       "1  1991,A Geological Perspective On Climatic-chan...      NaN          4   \n",
       "2  1991,A Spatial Model For Studying The Effects ...      NaN          4   \n",
       "3  1991,Abrupt Deep-sea Warming| Palaeoceanograph...      NaN          4   \n",
       "4  1991,Advance Of East Antarctic Outlet Glaciers...      NaN          4   \n",
       "\n",
       "  FullContent  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts.drop(columns='FullContent', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abstracts.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    511\n",
       "3    169\n",
       "2     52\n",
       "5     50\n",
       "7      8\n",
       "6      8\n",
       "1      3\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1991,Comparisons Of Observed Northern-hemisphe...</td>\n",
       "      <td>GEOPHYSICAL RESEARCH LETTERS, VOL. 18, NO. 7, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1991,Global Climate Change,Journal Of Engineer...</td>\n",
       "      <td>This chapter presents a bibliography of goal p...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1991,Global Warming - What Does The Science Te...</td>\n",
       "      <td>A review of goal programming formulations and ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1991,Global Warming As A Manifestation Of A Ra...</td>\n",
       "      <td>Global and hemispheric series of surface tempe...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>1992,Global Warming - A Reduced Threat,Bulleti...</td>\n",
       "      <td>One popular and apocalyptic vision of the worl...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "30   1991,Comparisons Of Observed Northern-hemisphe...   \n",
       "55   1991,Global Climate Change,Journal Of Engineer...   \n",
       "58   1991,Global Warming - What Does The Science Te...   \n",
       "64   1991,Global Warming As A Manifestation Of A Ra...   \n",
       "214  1992,Global Warming - A Reduced Threat,Bulleti...   \n",
       "\n",
       "                                              Abstract  sentiment  \n",
       "30   GEOPHYSICAL RESEARCH LETTERS, VOL. 18, NO. 7, ...          5  \n",
       "55   This chapter presents a bibliography of goal p...          6  \n",
       "58   A review of goal programming formulations and ...          6  \n",
       "64   Global and hemispheric series of surface tempe...          5  \n",
       "214  One popular and apocalyptic vision of the worl...          5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts.sentiment.replace(to_replace=[1,2,3],value=1,inplace=True)\n",
    "abstracts.sentiment.replace(to_replace=[5,6,7],value=0,inplace=True)\n",
    "abstracts.sentiment.replace(to_replace=[4],value=2,inplace=True)\n",
    "abstracts.sentiment.value_counts()\n",
    "abstracts['index']=abstracts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "balancedSet=abstracts[abstracts['sentiment']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(series, n):\n",
    "    np.random.seed()\n",
    "    return list(np.random.choice(series, size=n, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevantList = abstracts[abstracts.sentiment==2].agg(lambda x: sample(x, 66))['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "balancedSet=balancedSet.append(abstracts[abstracts['index'].isin(irrelevantList)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "supportingList=abstracts[abstracts.sentiment==1].agg(lambda x: sample(x, 66))['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "balancedSet=balancedSet.append(abstracts[abstracts['index'].isin(supportingList)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    66\n",
       "1    66\n",
       "0    66\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balancedSet['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1443.8181818181818\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(balancedSet['Abstract'].apply(len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 198 entries, 30 to 10191\n",
      "Data columns (total 4 columns):\n",
      "Title        198 non-null object\n",
      "Abstract     198 non-null object\n",
      "sentiment    198 non-null int64\n",
      "index        198 non-null int64\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 7.7+ KB\n"
     ]
    }
   ],
   "source": [
    "balancedSet.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "balancedSet.set_index(np.arange(0,198,1),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def abstract_cleaner(abstract):\n",
    "    #1. Remove HTML tags\n",
    "    abstract = bs.BeautifulSoup(abstract).text\n",
    "    #abstract = data['abstract'].apply(lambda text: bs.BeautifulSoup(text, 'html.parser').get_text())\n",
    "    \n",
    "    #2. Use regex to find emoticons\n",
    "    #emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', review)\n",
    "    \n",
    "    #3. Remove punctuation\n",
    "    abstract = re.sub(\"[^a-zA-Z0-9]\", \" \",abstract)\n",
    "    \n",
    "    #4. Tokenize into words (all lower case)\n",
    "    #abstract = abstract.str.lower()\n",
    "    abstract = abstract.lower().split()\n",
    "   \n",
    "    #5. Remove stopwords\n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    abstract = [w for w in abstract if not w in eng_stopwords]\n",
    "    \n",
    "    #6. Join the review to one sentence\n",
    "    #review = ' '.join(review+emoticons)\n",
    "    abstract = ' '.join(abstract)\n",
    "    # add emoticons to the end\n",
    "    return(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 152 ms, sys: 43.3 ms, total: 196 ms\n",
      "Wall time: 228 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_abstract = len(balancedSet)\n",
    "\n",
    "abstract_clean_original = []\n",
    "\n",
    "for i in range(0,num_abstract):\n",
    "    abstract_clean_original.append(abstract_cleaner(balancedSet['Abstract'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 542 ms, sys: 4.85 ms, total: 547 ms\n",
      "Wall time: 545 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Porter stemming on the results in review_clean_original\n",
    "\n",
    "abstract_clean_ps = []\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for i in range(0,num_abstract):\n",
    "    if( (i+1)%500 == 0 ):\n",
    "        # print progress\n",
    "        print(\"Done with %d reviews\" %(i+1)) \n",
    "    ps_stems = []\n",
    "    for w in abstract_clean_original[i].split():\n",
    "        if w == 'oed':\n",
    "            continue\n",
    "        ps_stems.append(ps.stem(w))\n",
    "    \n",
    "    abstract_clean_ps.append(' '.join(ps_stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.66 s, sys: 11.7 ms, total: 1.67 s\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Lemmatizer\n",
    "\n",
    "abstract_clean_wnl = []\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for i in range(0,num_abstract):\n",
    "    wnl_stems = []\n",
    "    token_tag = pos_tag(abstract_clean_original[i].split())\n",
    "    for pair in token_tag:\n",
    "        res = wnl.lemmatize(pair[0],pos=get_wordnet_pos(pair[1]))\n",
    "        wnl_stems.append(res)\n",
    "\n",
    "    abstract_clean_wnl.append(' '.join(wnl_stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics # for confusion matrix, accuracy score etc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "    abstract_clean_original, balancedSet['sentiment'], random_state=0, test_size=.2)\n",
    "\n",
    "\n",
    "# CountVectorizer can actucally handle a lot of the preprocessing for us\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.6 ms, sys: 1.61 ms, total: 34.2 ms\n",
      "Wall time: 33.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=300, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '100', '16', '20', '2000', '2004', '30', '40', 'accumulation', 'activity', 'addition', 'affect', 'air', 'al', 'also', 'although', 'among', 'amount', 'analysis', 'analyze', 'annual', 'anthropogenic', 'appear', 'application', 'approach', 'arctic', 'area', 'assessment', 'associate', 'atlantic', 'atmosphere', 'atmospheric', 'average', 'balance', 'base', 'basin', 'biomass', 'carbon', 'case', 'cause', 'century', 'ch4', 'change', 'china', 'circulation', 'climate', 'climatic', 'cloud', 'co2', 'compare', 'comparison', 'component', 'concentration', 'condition', 'consider', 'content', 'contrast', 'contribute', 'control', 'conventional', 'cool', 'coral', 'core', 'correlation', 'could', 'cover', 'crop', 'current', 'cycle', 'data', 'day', 'decrease', 'degree', 'derive', 'describe', 'determine', 'develop', 'development', 'difference', 'different', 'dioxide', 'distribution', 'dry', 'due', 'dynamic', 'earth', 'economic', 'ecosystem', 'effect', 'emission', 'energy', 'environmental', 'error', 'estimate', 'et', 'even', 'event', 'evidence', 'examine', 'expect', 'experiment', 'factor', 'feedback', 'field', 'find', 'first', 'flux', 'force', 'forest', 'fossil', 'four', 'frequency', 'future', 'gas', 'general', 'gerhard', 'give', 'glacial', 'glacier', 'global', 'great', 'greenhouse', 'grow', 'growth', 'gwp', 'heat', 'hemisphere', 'high', 'however', 'human', 'ice', 'impact', 'important', 'include', 'increase', 'indicate', 'induce', 'influence', 'information', 'investigate', 'ipcc', 'land', 'large', 'last', 'late', 'latitude', 'lead', 'less', 'level', 'long', 'low', 'mainly', 'major', 'make', 'management', 'marine', 'matter', 'may', 'mean', 'measure', 'measurement', 'method', 'might', 'mitigation', 'model', 'monsoon', 'n2o', 'natural', 'need', 'net', 'new', 'north', 'northern', 'nt', 'number', 'observation', 'observe', 'occur', 'ocean', 'one', 'order', 'organic', 'oscillation', 'pacific', 'paper', 'parameter', 'part', 'past', 'pattern', 'period', 'phase', 'plant', 'policy', 'population', 'positive', 'possible', 'potential', 'precipitation', 'predict', 'prediction', 'present', 'pressure', 'problem', 'process', 'produce', 'production', 'project', 'provide', 'rainfall', 'range', 'rate', 'recent', 'record', 'reduce', 'reduction', 'region', 'regional', 'relate', 'relationship', 'relatively', 'release', 'report', 'require', 'research', 'respectively', 'response', 'result', 'review', 'rise', 'role', 'satellite', 'scale', 'scenario', 'scientific', 'sea', 'season', 'series', 'several', 'shift', 'short', 'show', 'significant', 'significantly', 'similar', 'simulate', 'simulation', 'since', 'site', 'small', 'soil', 'solar', 'source', 'south', 'southern', 'specie', 'spring', 'state', 'stream', 'strong', 'structure', 'study', 'suggest', 'summer', 'support', 'surface', 'system', 'take', 'technology', 'temperature', 'term', 'thermal', 'three', 'thus', 'tillage', 'time', 'total', 'tree', 'trend', 'tropical', 'two', 'type', 'use', 'value', 'variability', 'variation', 'various', 'vegetation', 'warm', 'warming', 'water', 'wave', 'weather', 'well', 'winter', 'within', 'work', 'would', 'year', 'yield', 'yr']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.07286552e-03 3.35070286e-04 3.38727582e-03 5.41238065e-04\n",
      " 6.03306498e-04 1.26212575e-03 1.12532576e-03 5.92414634e-04\n",
      " 5.84577829e-04 1.37921630e-03 9.24412461e-05 1.63382912e-03\n",
      " 3.51293253e-03 2.77362585e-03 1.84497480e-03 0.00000000e+00\n",
      " 4.73141862e-03 3.17997021e-04 1.48229955e-03 5.75246321e-04\n",
      " 5.50302037e-03 1.77243504e-03 9.42777196e-03 1.02604524e-03\n",
      " 5.76795997e-04 1.74629715e-03 0.00000000e+00 5.59027538e-03\n",
      " 3.61769970e-03 0.00000000e+00 4.51572900e-03 9.57927130e-04\n",
      " 6.92091750e-03 2.88358574e-03 2.60915120e-03 7.49701145e-04\n",
      " 9.48247551e-04 2.22613278e-03 6.09361522e-04 0.00000000e+00\n",
      " 1.27414217e-05 1.97423521e-02 0.00000000e+00 4.93603800e-03\n",
      " 4.14690773e-03 1.06679407e-02 3.53294890e-03 3.28116903e-03\n",
      " 9.63305230e-04 5.99769629e-03 3.76656004e-03 1.45817070e-02\n",
      " 1.90109493e-04 3.36506986e-03 2.76668835e-03 4.72063575e-03\n",
      " 3.17175940e-03 2.38506018e-03 1.26848703e-03 5.64677332e-04\n",
      " 2.90763340e-03 7.06042510e-04 7.92153339e-03 2.53249936e-03\n",
      " 2.43009089e-03 6.26530940e-04 6.61949288e-03 2.38605205e-03\n",
      " 4.32305895e-03 6.25688095e-03 1.91375969e-04 2.80500013e-03\n",
      " 1.49902760e-03 3.08515656e-03 2.26958298e-03 5.39133415e-03\n",
      " 2.64643616e-03 2.69433352e-03 6.74065749e-03 1.04302978e-02\n",
      " 1.32038270e-03 2.12710754e-03 4.85372125e-03 1.48907655e-03\n",
      " 2.45837805e-02 1.92560413e-02 4.80833449e-03 4.60426949e-03\n",
      " 1.60468457e-03 2.43950645e-03 5.52806551e-04 2.26022857e-03\n",
      " 1.31674069e-03 2.02282444e-03 2.56410256e-04 1.41694924e-03\n",
      " 5.44752630e-03 4.56649519e-04 7.91535332e-04 3.52830679e-03\n",
      " 2.92667147e-03 5.57387430e-04 5.98400513e-03 2.53838664e-03\n",
      " 7.63473007e-03 3.63849146e-03 8.46465155e-03 2.28062536e-03\n",
      " 0.00000000e+00 1.25396521e-03 0.00000000e+00 1.23906075e-03\n",
      " 2.16990167e-02 1.88065084e-03 6.78665249e-03 2.19720662e-03\n",
      " 1.18039850e-03 7.80840807e-04 1.77663413e-03 5.78779192e-03\n",
      " 3.64324654e-03 4.73990705e-03 6.29057620e-04 2.99111164e-03\n",
      " 3.47252123e-03 9.90529728e-03 3.54764149e-03 4.73805666e-03\n",
      " 1.06822054e-02 3.12800784e-03 4.67895651e-03 9.26280222e-04\n",
      " 1.93780153e-03 0.00000000e+00 5.24238703e-04 1.84359877e-03\n",
      " 6.58916382e-04 2.60634118e-03 3.06108510e-03 4.13348547e-03\n",
      " 2.09162878e-03 2.28961118e-03 4.62301994e-03 9.38436739e-04\n",
      " 2.87359946e-03 1.98022781e-03 1.77785557e-03 6.27205552e-03\n",
      " 2.70625924e-03 2.46612987e-03 1.66234137e-03 6.09070276e-04\n",
      " 4.53765258e-03 2.53614077e-03 7.39699121e-03 3.69187989e-03\n",
      " 1.41372372e-03 2.27548289e-03 6.54547520e-04 9.20836243e-04\n",
      " 3.65116433e-03 3.64015474e-03 0.00000000e+00 6.67738429e-04\n",
      " 3.71547749e-03 5.60322474e-04 2.60185034e-03 1.64543826e-03\n",
      " 3.06710598e-03 5.88086379e-03 0.00000000e+00 1.74742416e-03\n",
      " 5.15577791e-03 4.57908292e-03 1.42374159e-03 1.53509060e-03\n",
      " 3.41748250e-03 1.54040843e-03 2.80181709e-04 4.89346927e-03\n",
      " 2.12660616e-03 8.09347200e-03 9.05659091e-04 1.23985190e-03\n",
      " 4.72401105e-03 8.96344751e-04 5.86982229e-03 5.38374174e-03\n",
      " 5.02618198e-03 1.93790636e-03 8.45008619e-04 2.40137988e-03\n",
      " 3.72417897e-03 1.15751004e-02 2.22068560e-03 2.71571954e-03\n",
      " 7.48645134e-04 1.35766965e-03 1.64545185e-03 4.82302296e-03\n",
      " 3.02639631e-03 1.79136419e-03 1.93785153e-03 5.69416490e-03\n",
      " 1.86729632e-03 3.81727413e-03 1.71615576e-03 3.22600071e-03\n",
      " 1.22557967e-03 4.94595897e-03 1.56490178e-03 4.71268243e-03\n",
      " 2.70423534e-03 4.05580538e-03 2.95395962e-03 3.43249694e-03\n",
      " 1.99847975e-03 3.83230262e-03 6.97743133e-04 4.07700878e-03\n",
      " 2.69472058e-03 4.58529022e-03 3.28323124e-03 3.66242530e-03\n",
      " 5.45495862e-03 2.26094435e-03 3.64269998e-03 2.95652995e-03\n",
      " 2.53451820e-03 4.37379214e-03 4.49768802e-03 3.33252983e-04\n",
      " 2.24191595e-03 6.04032132e-03 8.59819464e-04 2.83392898e-04\n",
      " 2.37091754e-03 1.65524350e-03 2.54653202e-03 8.16402624e-03\n",
      " 4.05969820e-03 3.28636584e-03 1.84819746e-03 2.62029068e-03\n",
      " 1.54290110e-03 6.40030290e-03 2.18730725e-03 9.01203086e-03\n",
      " 3.08407504e-03 3.82501615e-02 1.51202366e-03 1.90020545e-03\n",
      " 1.90426627e-03 2.70223533e-03 1.28942113e-02 3.77746146e-04\n",
      " 1.21426947e-03 1.19176320e-03 1.69328702e-03 3.25524740e-03\n",
      " 3.16967890e-03 2.40020568e-03 6.91486168e-03 3.61745154e-03\n",
      " 1.92602161e-03 1.08108266e-02 2.11631085e-03 3.35474401e-03\n",
      " 1.98376674e-03 4.09689330e-03 2.33526922e-03 2.20410217e-03\n",
      " 2.73952882e-03 5.09642869e-04 4.72469481e-04 2.54828434e-04\n",
      " 6.04116152e-03 1.67965437e-03 1.74006406e-03 2.95116448e-03\n",
      " 2.76737147e-03 2.35401670e-03 4.51454258e-03 1.51857231e-03\n",
      " 2.35239353e-03 2.77728189e-03 2.47909399e-03 4.80662229e-03\n",
      " 9.14199165e-04 1.39620947e-03 1.20976566e-03 1.28959179e-02\n",
      " 5.70602767e-03 3.34090647e-03 3.29292287e-03 1.80615057e-03\n",
      " 3.05884539e-03 4.26620044e-03 3.63273457e-03 1.30843481e-03]\n"
     ]
    }
   ],
   "source": [
    "importances = forest.feature_importances_\n",
    "# returns relative importance of all features.\n",
    "# they are in the order of the columns\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bag = vectorizer.transform(X_train) #transform to a feature matrix\n",
    "test_bag = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 66.8 ms, sys: 1.92 ms, total: 68.7 ms\n",
      "Wall time: 68.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "forest = forest.fit(train_bag, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = forest.predict(train_bag)\n",
    "valid_predictions = forest.predict(test_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_train,train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.775"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test,valid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics # for confusion matrix, accuracy score etc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(\\\n",
    "    abstract_clean_wnl, balancedSet['sentiment'], random_state=0, test_size=.2)\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.9 ms, sys: 1.7 ms, total: 32.6 ms\n",
      "Wall time: 31.3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=300, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer.fit(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '100', '16', '20', '2000', '2004', '30', '40', 'accumulation', 'activity', 'addition', 'affect', 'air', 'al', 'also', 'although', 'among', 'amount', 'analysis', 'analyze', 'annual', 'anthropogenic', 'appear', 'application', 'approach', 'arctic', 'area', 'assessment', 'associate', 'atlantic', 'atmosphere', 'atmospheric', 'average', 'balance', 'base', 'basin', 'biomass', 'carbon', 'case', 'cause', 'century', 'ch4', 'change', 'china', 'circulation', 'climate', 'climatic', 'cloud', 'co2', 'compare', 'comparison', 'component', 'concentration', 'condition', 'consider', 'content', 'contrast', 'contribute', 'control', 'conventional', 'cool', 'coral', 'core', 'correlation', 'could', 'cover', 'crop', 'current', 'cycle', 'data', 'day', 'decrease', 'degree', 'derive', 'describe', 'determine', 'develop', 'development', 'difference', 'different', 'dioxide', 'distribution', 'dry', 'due', 'dynamic', 'earth', 'economic', 'ecosystem', 'effect', 'emission', 'energy', 'environmental', 'error', 'estimate', 'et', 'even', 'event', 'evidence', 'examine', 'expect', 'experiment', 'factor', 'feedback', 'field', 'find', 'first', 'flux', 'force', 'forest', 'fossil', 'four', 'frequency', 'future', 'gas', 'general', 'gerhard', 'give', 'glacial', 'glacier', 'global', 'great', 'greenhouse', 'grow', 'growth', 'gwp', 'heat', 'hemisphere', 'high', 'however', 'human', 'ice', 'impact', 'important', 'include', 'increase', 'indicate', 'induce', 'influence', 'information', 'investigate', 'ipcc', 'land', 'large', 'last', 'late', 'latitude', 'lead', 'less', 'level', 'long', 'low', 'mainly', 'major', 'make', 'management', 'marine', 'matter', 'may', 'mean', 'measure', 'measurement', 'method', 'might', 'mitigation', 'model', 'monsoon', 'n2o', 'natural', 'need', 'net', 'new', 'north', 'northern', 'nt', 'number', 'observation', 'observe', 'occur', 'ocean', 'one', 'order', 'organic', 'oscillation', 'pacific', 'paper', 'parameter', 'part', 'past', 'pattern', 'period', 'phase', 'plant', 'policy', 'population', 'positive', 'possible', 'potential', 'precipitation', 'predict', 'prediction', 'present', 'pressure', 'problem', 'process', 'produce', 'production', 'project', 'provide', 'rainfall', 'range', 'rate', 'recent', 'record', 'reduce', 'reduction', 'region', 'regional', 'relate', 'relationship', 'relatively', 'release', 'report', 'require', 'research', 'respectively', 'response', 'result', 'review', 'rise', 'role', 'satellite', 'scale', 'scenario', 'scientific', 'sea', 'season', 'series', 'several', 'shift', 'short', 'show', 'significant', 'significantly', 'similar', 'simulate', 'simulation', 'since', 'site', 'small', 'soil', 'solar', 'source', 'south', 'southern', 'specie', 'spring', 'state', 'stream', 'strong', 'structure', 'study', 'suggest', 'summer', 'support', 'surface', 'system', 'take', 'technology', 'temperature', 'term', 'thermal', 'three', 'thus', 'tillage', 'time', 'total', 'tree', 'trend', 'tropical', 'two', 'type', 'use', 'value', 'variability', 'variation', 'various', 'vegetation', 'warm', 'warming', 'water', 'wave', 'weather', 'well', 'winter', 'within', 'work', 'would', 'year', 'yield', 'yr']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bag = vectorizer.transform(X_train1) #transform to a feature matrix\n",
    "test_bag = vectorizer.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_train1,train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.775"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test1,valid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def predict_sentiment(cleaned_reviews, y=balancedSet[\"sentiment\"]):\n",
    "\n",
    "    print(\"Creating the bag of words model!\\n\")\n",
    "    # CountVectorizer\" is scikit-learn's bag of words tool, here we show more keywords \n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = 300) \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "    cleaned_reviews, y, random_state=0, test_size=.2)\n",
    "    \n",
    "    train_bag = vectorizer.fit_transform(X_train).toarray()\n",
    "    test_bag = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "    # You can extract the vocabulary created by CountVectorizer\n",
    "    # by running print(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 75 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_bag, y_train)\n",
    "\n",
    "\n",
    "    train_predictions = forest.predict(train_bag)\n",
    "    test_predictions = forest.predict(test_bag)\n",
    "    \n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    print(\"The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n",
    "    \n",
    "    return(forest,vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.7\n",
      "Porter Stemmer\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.65\n",
      "Lemmatizing\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.625\n"
     ]
    }
   ],
   "source": [
    "print('Original Reviews')\n",
    "forest1,vec1 = predict_sentiment(abstract_clean_original)\n",
    "print('Porter Stemmer')\n",
    "forest2,vec2 = predict_sentiment(abstract_clean_ps)\n",
    "print('Lemmatizing')\n",
    "forest3,vec3 = predict_sentiment(abstract_clean_wnl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
